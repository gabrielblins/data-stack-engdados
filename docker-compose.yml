version: "3"
x-minio-common: &minio-common
    image: quay.io/minio/minio:RELEASE.2023-02-22T18-23-45Z
    command: server --console-address ":9001" http://minio{1...4}/data{1...2}
    expose:
      - "9000"
      - "9001"
    environment:
      - MINIO_ROOT_USER=airflow
      - MINIO_ROOT_PASSWORD=sample_key
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - server
    
services:

    jupyter:
        build: ./jupyter
        container_name: JupyterLab
        environment:
            - JUPYTER_ENABLE_LAB="yes"
            - PYSPARK_SUBMIT_ARGS= --master spark://spark-master:7077 --packages com.amazonaws:aws-java-sdk-bundle:1.11.819,org.apache.hadoop:hadoop-aws:3.2.1 pyspark-shell
            - AWS_ACCESS_KEY_ID=airflow
            - AWS_SECRET_ACCESS_KEY=sample_key
            - MLFLOW_S3_ENDPOINT_URL=http://minio1:9000
        ports:
            - "9999:9999"
        entrypoint:
            - bash
        command:
            - -c
            - |
                jupyter lab --ip=0.0.0.0 --port=9999 --allow-root --no-browser --NotebookApp.notebook_dir='/home/jovyan/work' --NotebookApp.token=''
        volumes:
            - ./src/notebooks:/home/jovyan/work
        networks:
            - server

    airflow-webserver:
        hostname: airflow
        container_name: airflow
        image: andrejunior/airflow-spark:latest
        restart: always
        networks:
            - server
        depends_on:
            - postgres
            - spark-master
            - spark-worker
        environment:   
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
            - LOAD_EX=n
            - EXECUTOR=Local    
        volumes:
            - airflow-data:/usr/local/airflow/data
            - ./src/dags:/usr/local/airflow/dags
            - ./src/spark/applications:/usr/local/spark/applications            
            - ./src/spark/assets:/usr/local/spark/assets     
        ports:
            - "8085:8080"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    postgres:
        hostname: postgres
        container_name: postgres
        image: 'postgres:14-bullseye'
        environment:
            POSTGRES_USER: 'airflow'
            POSTGRES_PASSWORD: 'airflow'
            POSTGRES_DB: 'airflow'
            PGDATA: /data/postgres
        volumes:
            - postgres:/data/postgres
        ports:
            - "5432:5432"
        networks:
            - server
        restart: on-failure
        healthcheck:
            test: ["CMD", "pg_isready"]
            interval: 60s
            timeout: 20s
            retries: 3
        deploy:
          resources:
            limits:
              memory: 400MB

#    minio:
#        #hostname: bucket 
#        #container_name: bucket
#        image: 'bitnami/minio:latest'
#        environment:
#            MINIO_ROOT_USER: airflow
#            MINIO_ROOT_PASSWORD: airflow
#        ports:
#            - '9000:9000'
#            - '9001:9001'
#        volumes:
#            - minio_data:/data
#        networks:
#            - airflow
#        healthcheck:
#            test: ["CMD", "curl", "-f", "http://minio:9000/minio/health/live"]
#            interval: 60s
#            timeout: 20s
#            retries: 3
#        deploy:
#          resources:
#            limits:
#              memory: 400MB

    create_s3_buckets:
        container_name: CreateS3Bucket
        image: minio/mc
        depends_on:
            - minio1
            - minio2
            - minio3
            - minio4
        entrypoint: >
            /bin/sh -c "
            until (/usr/bin/mc alias set minio http://minio1:9000 'airflow' 'sample_key') do echo '...waiting...' && sleep 1; done;
            /usr/bin/mc mb minio/raw;
            /usr/bin/mc mb minio/silver;
            /usr/bin/mc mb minio/gold;
            exit 0;
            "
        networks:
            - server
      
    spark-master:        
        image: bitnami/spark:3.2.1
        # https://docs.bitnami.com/tutorials/work-with-non-root-containers/
        user: root 
        hostname: spark
        container_name: spark
        networks:
            - server
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./src/spark/applications:/usr/local/spark/applications            
            - ./src/spark/assets:/usr/local/spark/assets 
        ports:
            - "8081:8080"
            - "7077:7077"
        deploy:
          resources:
            limits:
              memory: 500MB

    spark-worker:
        image: bitnami/spark:3.2.1
        user: root
        hostname: spark-worker
        container_name: spark-worker
        networks:
            - server
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./src/spark/applications:/usr/local/spark/applications            
            - ./src/spark/assets:/usr/local/spark/assets 
        depends_on:
            - spark-master
        deploy:
          resources:
            limits:
              memory: 1GB
    
    minio1:
        <<: *minio-common
        hostname: minio1
        volumes:
            - data1-1:/data1
            - data1-2:/data2

    minio2:
        <<: *minio-common
        hostname: minio2
        volumes:
            - data2-1:/data1
            - data2-2:/data2

    minio3:
        <<: *minio-common
        hostname: minio3
        volumes:
            - data3-1:/data1
            - data3-2:/data2

    minio4:
        <<: *minio-common
        hostname: minio4
        volumes:
            - data4-1:/data1
            - data4-2:/data2

    nginx:
        image: nginx:1.19.2-alpine
        hostname: nginx
        volumes:
            - ./nginx.conf:/etc/nginx/nginx.conf:ro
        ports:
            - "9000:9000"
            - "9001:9001"
        depends_on:
            - minio1
            - minio2
            - minio3
            - minio4
        networks:
            - server

networks:
  server:

volumes:
    postgres:
    airflow-data:
    data1-1:
    data1-2:
    data2-1:
    data2-2:
    data3-1:
    data3-2:
    data4-1:
    data4-2:
