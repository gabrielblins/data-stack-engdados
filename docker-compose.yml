version: "3"
x-minio-common: &minio-common
    image: quay.io/minio/minio:RELEASE.2023-02-22T18-23-45Z
    command: server --console-address ":9001" http://minio{1...4}/data{1...2}
    expose:
      - "9000"
      - "9001"
    environment:
      - MINIO_ROOT_USER=airflow
      - MINIO_ROOT_PASSWORD=sample_key
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - server
    
services:

    jupyter:
        build: ./jupyter
        container_name: JupyterLab
        environment:
            - JUPYTER_ENABLE_LAB="yes"
            - PYSPARK_SUBMIT_ARGS= --master spark://spark-master:7077 --packages com.amazonaws:aws-java-sdk-bundle:1.11.819,org.apache.hadoop:hadoop-aws:3.2.3 pyspark-shell
            - AWS_ACCESS_KEY_ID=airflow
            - AWS_SECRET_ACCESS_KEY=sample_key
            - MLFLOW_S3_ENDPOINT_URL=http://minio1:9000
        ports:
            - "9999:9999"
        entrypoint:
            - bash
        command:
            - -c
            - |
                jupyter lab --ip=0.0.0.0 --port=9999 --allow-root --no-browser --NotebookApp.notebook_dir='/home/jovyan/work' --NotebookApp.token=''
        volumes:
            - ./src/notebooks:/home/jovyan/work
        networks:
            - server

    airflow-webserver:
        hostname: airflow
        container_name: airflow
        image: andrejunior/airflow-spark:latest
        networks:
            - server
        depends_on:
            - postgres
            - spark-master
            - spark-worker-a
            - spark-worker-b
        environment:   
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
            - LOAD_EX=n
            - EXECUTOR=Local
            - PYSPARK_SUBMIT_ARGS= --master spark://spark-master:7077 --packages com.amazonaws:aws-java-sdk-bundle:1.11.819
        volumes:
            - airflow-data:/usr/local/airflow/data
            - ./src/dags:/usr/local/airflow/dags
            - ./src/spark/applications:/usr/local/spark/applications            
            - ./src/spark/assets:/usr/local/spark/assets     
        ports:
            - "8085:8080"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
    

    postgres:
        hostname: postgres
        container_name: postgres
        image: 'postgres:14-bullseye'
        environment:
            POSTGRES_USER: 'airflow'
            POSTGRES_PASSWORD: 'airflow'
            POSTGRES_DB: 'airflow'
            PGDATA: /data/postgres
        volumes:
            - postgres:/data/postgres
        ports:
            - "5432:5432"
        networks:
            - server
        restart: on-failure
        healthcheck:
            test: ["CMD", "pg_isready"]
            interval: 60s
            timeout: 20s
            retries: 3
        deploy:
          resources:
            limits:
              memory: 400MB

#    minio:
#        #hostname: bucket 
#        #container_name: bucket
#        image: 'bitnami/minio:latest'
#        environment:
#            MINIO_ROOT_USER: airflow
#            MINIO_ROOT_PASSWORD: airflow
#        ports:
#            - '9000:9000'
#            - '9001:9001'
#        volumes:
#            - minio_data:/data
#        networks:
#            - airflow
#        healthcheck:
#            test: ["CMD", "curl", "-f", "http://minio:9000/minio/health/live"]
#            interval: 60s
#            timeout: 20s
#            retries: 3
#        deploy:
#          resources:
#            limits:
#              memory: 400MB

    create_s3_buckets:
        container_name: CreateS3Bucket
        image: minio/mc
        depends_on:
            - minio1
            - minio2
            - minio3
            - minio4
        entrypoint: >
            /bin/sh -c "
            until (/usr/bin/mc alias set minio http://minio1:9000 'airflow' 'sample_key') do echo '...waiting...' && sleep 1; done;
            /usr/bin/mc mb minio/raw;
            /usr/bin/mc mb minio/silver;
            /usr/bin/mc mb minio/gold;
            exit 0;
            "
        networks:
            - server

    spark-master:
        container_name: SparkMaster
        build:
            context: ./spark_docker/build
            dockerfile: Dockerfile
        ports:
            - "9090:8080"
            - "7077:7077"
        volumes:
            - ./notebooks:/home/jovyan/work:rw
            - ./src/spark/applications:/usr/local/spark/applications            
            - ./src/spark/assets:/usr/local/spark/assets
        environment:
            - SPARK_LOCAL_IP=spark-master
            - SPARK_WORKLOAD=master
        networks:
            - server

    spark-worker-a:
        container_name: SparkWorkerA
        build:
            context: ./spark_docker/build
            dockerfile: Dockerfile
        ports:
            - "9091:8080"
            - "7000:7000"
        depends_on:
            - spark-master
        environment:
            - SPARK_MASTER=spark://spark-master:7077
            - SPARK_WORKER_CORES=2
            - SPARK_WORKER_MEMORY=2G
            - SPARK_DRIVER_MEMORY=2G
            - SPARK_EXECUTOR_MEMORY=2G
            - SPARK_WORKLOAD=worker
            - SPARK_LOCAL_IP=spark-worker-a
        volumes:
            - ./src/notebooks:/home/jovyan/work:rw
            - ./src/spark/applications:/usr/local/spark/applications            
            - ./src/spark/assets:/usr/local/spark/assets
        networks:
            - server

    spark-worker-b:
        container_name: SparkWorkerB
        build:
            context: ./spark_docker/build
            dockerfile: Dockerfile
        ports:
            - "9092:8080"
            - "7001:7000"
        depends_on:
            - spark-master
        environment:
            - SPARK_MASTER=spark://spark-master:7077
            - SPARK_WORKER_CORES=2
            - SPARK_WORKER_MEMORY=2G
            - SPARK_DRIVER_MEMORY=2G
            - SPARK_EXECUTOR_MEMORY=2G
            - SPARK_WORKLOAD=worker
            - SPARK_LOCAL_IP=spark-worker-b
        volumes:
            - ./src/notebooks:/home/jovyan/work:rw
            - ./src/spark/applications:/usr/local/spark/applications            
            - ./src/spark/assets:/usr/local/spark/assets
        networks:
            - server  
    # spark-master:        
    #     image: bitnami/spark:3.2.1
    #     # https://docs.bitnami.com/tutorials/work-with-non-root-containers/
    #     user: root 
    #     hostname: spark
    #     container_name: spark
    #     networks:
    #         - server
    #     environment:
    #         - SPARK_MODE=master
    #         - SPARK_RPC_AUTHENTICATION_ENABLED=no
    #         - SPARK_RPC_ENCRYPTION_ENABLED=no
    #         - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
    #         - SPARK_SSL_ENABLED=no
    #     volumes:
    #         - ./src/spark/applications:/usr/local/spark/applications            
    #         - ./src/spark/assets:/usr/local/spark/assets 
    #     ports:
    #         - "8081:8080"
    #         - "7077:7077"
    #     deploy:
    #       resources:
    #         limits:
    #           memory: 500MB

    # spark-worker:
    #     image: bitnami/spark:3.2.1
    #     user: root
    #     hostname: spark-worker
    #     container_name: spark-worker
    #     networks:
    #         - server
    #     environment:
    #         - SPARK_MODE=worker
    #         - SPARK_MASTER_URL=spark://spark:7077
    #         - SPARK_WORKER_MEMORY=1G
    #         - SPARK_WORKER_CORES=1
    #         - SPARK_RPC_AUTHENTICATION_ENABLED=no
    #         - SPARK_RPC_ENCRYPTION_ENABLED=no
    #         - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
    #         - SPARK_SSL_ENABLED=no
    #     volumes:
    #         - ./src/spark/applications:/usr/local/spark/applications            
    #         - ./src/spark/assets:/usr/local/spark/assets 
    #     depends_on:
    #         - spark-master
    #     deploy:
    #       resources:
    #         limits:
    #           memory: 1GB
    
    minio1:
        <<: *minio-common
        hostname: minio1
        volumes:
            - data1-1:/data1
            - data1-2:/data2

    minio2:
        <<: *minio-common
        hostname: minio2
        volumes:
            - data2-1:/data1
            - data2-2:/data2

    minio3:
        <<: *minio-common
        hostname: minio3
        volumes:
            - data3-1:/data1
            - data3-2:/data2

    minio4:
        <<: *minio-common
        hostname: minio4
        volumes:
            - data4-1:/data1
            - data4-2:/data2

    nginx:
        image: nginx:1.19.2-alpine
        hostname: nginx
        volumes:
            - ./nginx.conf:/etc/nginx/nginx.conf:ro
        ports:
            - "9000:9000"
            - "9001:9001"
        depends_on:
            - minio1
            - minio2
            - minio3
            - minio4
        networks:
            - server

networks:
  server:

volumes:
    postgres:
    airflow-data:
    data1-1:
    data1-2:
    data2-1:
    data2-2:
    data3-1:
    data3-2:
    data4-1:
    data4-2:
